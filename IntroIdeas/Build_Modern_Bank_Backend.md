我是如何构建现代银行后端系统
我们是如何搭建现代银行后端的

在Monzo(是什么？)，我们在从零开始构建一个银行系统。我们的系统无论发生什么都得7*24小时运行，在全球可升级为上百万顾客，并且十分可扩展。这是系列的第一篇，关于我们的平台，解释我们是怎么构建系统来满足现代、开源技术的。

开始，我们构建了作为分布式微服务的合集的系统，对于一个初创公司，这个架构十分不寻常的，大多数公司开始一个中心化的应用，使用有名的框架和关系数据库。

但有野心构建当前账户世界上，并且扩展上百万用户，我们知道我们要成为最好的技术的存储。每天批处理过程，单点错误，和维护窗口在世界上希望得到7*24的顾客都不能接受的。一直在线来访问他们的钱，使得我们推出了新特性，以小时度量，而不是月。

大型互联网公司例如Amazon， Netflix和Twitter显示单块代码库不能扩展为大量的用户，关键性的大量开发者，它允许如果我们想要运作在许多市场，每个都有独立的需求，我们将某天需要很多团队，每个都工作在这个产品的不同方面。这些团队需要控制他们自己的开发部署和扩展，而不用和其他团队协调他们的改动。简言之，开发过程需要是分布式的和解耦的，就像我们的软件。

刚开始我们只有三个后端开发者，我们需要实际一点。我们选择了一些相似的技术，带我们到正确的方向，并且开始构建一个产品，知道未来的我们总是可以改变这些事情。

当我们上线了Monzo beta版，我们的后端发展为将近100个服务（现在有150个），而我们的工程师团队也在增长。我们知道我们的银行证书迫在眉睫，感觉是一个重新考虑我们的架构选择的时机了。我们开始了工程作坊，并且关注在少数几个方面：

1. 集群管理
    我们需要一个有效的、自动化的方式来管理我们大量的服务器，在其间分布式工作，并且反应为机器错误。
2. 精通服务
    Go非常适合构建微服务，并且可能保留了在Monzo的领域语言，但是专门使用它意味着我们不能使用其他语言开发的工具了。
3. RPC传输
    随着跨主机、跨数据中心甚至跨大陆的大量服务的分布，我们的系统强烈地依赖于RPC层的成功开始导致组件错误、小量延迟并且帮我们理解了它运行时的表现。这层使得我们在我们薄弱的地点进行了加强。
4. 异步消息
    为了是我们的后端表现得更好和保留，我们使用了消息队列来将后台工作入队列，这些队列提供强保证，工作可以被入队列和出队列，并且消息永不丢失。

集群管理
如果我们想构建一个容错、弹性扩展系统，它跟随着我们在硬件失败或随着用户需求的时候添加或移除服务器。每个主机运行一个服务是浪费的（服务可能并不需要这个主机的所有资源），并且传统的途径来手动在主机间划分服务是乏味的和难以扩展的。

集群调度被设计为抽象应用，远离它们所运行的硬件。它们使用算法来调度应用到主机上，根据它们的资源需求。扩展它们，并且当它们失效的时候替换它们。我们的终极目标是运行我们整个系统在一个调度器上，意味着我们要将有状态的服务部署为无状态的应用。

容器化，和Docker，最近难以忽略。在这后面，它们只是结合了Linux内核技术和一种镜像格式，但是提供了强有力的抽象。通过打包一个应用和它的依赖到一个单独的、标准化的镜像里，这个应用就与操作系统解耦了，并且与这个主机上的其他应用隔离了。一个集群管理要专门处理容器，很有意义。

在使用了一年Mesos和Marathon后，我们决定转换为Kubernetes，部署在一组CoreOS机器上。它与Docker集成的非常紧密，并且是谷歌长期在全球范围生产中运行容器大量经验的产物。部署Kubernetes在AWS的一个高可用配置不适合心隐隐，并且需要你熟悉内部。但是我们对结果非常满意。我们经常杀掉生产中的主机，并且Kubernetes快速重新调度应用来适应。事实上，我们很快部署了一些就像Netflix的Simian Army来故意制造一些错误来保证我们的系统透明地容错。

另外为了提升我们的恢复能力，Kubernetes给我们大量的节省。我们的生产基础设施仅占在转换为Kubernetes之前的1/4开销。为了证明一个例子为什么，考虑我们的构建系统。之前，我们运行几个非常结实的Jenkins主机献给这个任务，这是低效的且昂贵的。现在， 构建任务运行在Kubernetes之下，使用我们基础设施里的空余的空间，这是免费的。资源收集和限制保证了资源敏感，但是低优先级任务就像这些不会影响关键性能，面向用户的服务

多语言服务
Go使得构建低延迟、高效、高并发服务器变得容易，但是没有一种语言的生态系统有我们构建银行所需要的每个工具。我们需要能够使用大范围开源（和商务化）的工具，雇佣多种多样技能集的开发者，并且当技术改变时进化。

Docker允许我们简单地打包和发布应用而无视它们是由什么语言编写的，或它们有什么样的依赖。共同的经验提倡抽象共享代码到库里。但是在多语言系统里，这是无效的。如果我们有复制和维护上千甚至成百上千行共享代码，每次我们使用新语言时，这将很快变得不可控。

取代的是，我们压缩了共享代码为服务，提升自身的能力和品质。为了获取一个分布式锁，一个服务可以调用一个锁服务，它的前面是etcd通过"vanilla"RPC。我们考虑构建服务就像这些来面对所有的共享基础设施（包括数据库和消息队列）。所有我们可以减少每种语言需要的代码为每个客户端需要调用RPC刚好的量。这种方法允许我们构建服务用java，python和scala。

通过RPC为公共组件，我们仍然得到代码复用的好处而不用考虑共享代码带来的紧耦合。它实际上变得更简单来构建工具，应用配额、限流和访问控制。

RPC
我们想要用许多语言编写服务，无论什么协议我们选择必须支持大量的语言。HTTP是显而易见的赢家：每种语言都有标准库的实现。

然而， 依然有一大票功能你在RPC层需要的来构建鲁棒性微服务平台。为了提及一些更多的兴趣：

* 负载均衡： 大多数HTTP客户端库可以进行基于DNS的轮询负载均衡。但这是迟钝的设施。理想的，一个负载均衡器需要选择最合适的主机来减少错误和延迟，即是，即使在出现了失败和慢副本（运行较慢的服务器），（负载均衡）系统仍然运行且快速。
* 自动重试：一个分布式系统中，错误是不可避免的。如果一个下游服务失败了来完成一个幂等的请求，它可以被安全地在另一个副本上重试，保证系统作为一个整体恢复甚至出现了一些失败的组件。
* 连接池：为每个请求打开一个连接造成巨大的延迟负面效应，理想的，请求应该被操作到一些已存的连接上。
* 路由：非常有用有能力来改变运行时RPC系统的表现。例如，当我们部署一个服务的新版本，我们可能需要发送一部分全部的流量给它来检查它是否如我们所预期的表现，在增加到100%之前。这路由技术也可以用在内部测试：代替全部独立的分期系统，它变得可能来分期独立的生产上为特定用户的服务。

四下看去，Finagle是最有经验的RPC系统了。它有我们需要的和更多的功能，并且它非常整洁，模块化架构。在Twitter的生产中运行了多年，它被带到世界上最大的微服务部署里。有幸得到它，linkerd，一个out-of-process带来建立在Finagle之上，今年早些时候发布，意味着我们可以利用所有的这些特性而不用在JVM上写我们的应用。

代替直接与其他服务器交互，我们的服务与linkerd的本地副本交互，它路由请求到下游节点，选择使用一个Power of Two Choices 和Peak EWMA负载均衡器。当部分幂等请求失败，它们自动重试（预算内）。这些复杂的逻辑之间没有必须包含在独立服务里的。意味着我们自由地写我们的服务用任何语言而不用维护这些RPC库。

我们部署Linkerd作为Kubernetes上的一个DaemonSet，所以服务总是在本地localhost与Linkerd进行交互，它转发请求。在一些情况里，一个RPC将用户实际穿过网络如果通信服务双方的副本在一个主机上。当我们部署linkerd后我们发现和贡献了Kubernetes集成商的一些修补了bug。

异步消息
许多工作在后端的是异步的。例如，即使是端到端支付过程花费了不到一秒，我们想要响应为支付网络在实际微秒内来提升或减少一个Monzo卡的事物。丰富商户数据，发送推消息，甚至插入交互到用户的feed异步发生。

除了这些异步，一些步骤是非常重要的，并且永不跳过。即使一个错误发生，不能被自动恢复，它必须可能对我们来修复问题和恢复过程。这给我们几个需求对我们的异步架构：

* 高可用：消息发布者必须能够fire and forget消息到队列，无视错误节点或状态下游消息消费者
* 可扩展：我们必须能够增加消息队列的容量无需终端服务和无需升级硬件，消息队列它自身必须水平扩展，就像我们其他的服务一样
* 持久化：如果一节点的消息队列失效，我们不能丢失数据，相似地，如果一个消息消费者失效，它可以重新发送消息和重试。
* 回放：回放消息流从一个历史点很有用，所以新过程可以运行和反复运行旧数据
* 至少一次更新：所有消息必须发送到它们的消费者，并且及时一般可能来发送消息精确地一次，正常模式操作不应该消息发送多次。

与其他的消息队列相比，Kafka看起来自然适合于这些需求，它的架构是不同寻常的消息队列，它实际上是一个复制的commit log，但这使得它很适合我们的用例。它的复制和分块设计意味着它可能忍受节点错误和扩展不需要中断服务。它的消费者设计也很有趣，当最多的系统维护消息队列给每个消费者，Kafka消费者只光标移动到消息日志，这使得pub/sub系统更便宜。并且因为消息被保存一段时间，我们可以回放早些时间回到特定的服务。

我希望这篇文章给你启示关于我们如何思考后端架构，我几乎刮擦我的平台表面，这里大量更多来写在未来的博文里，例如我们如何存储数据，我们如何管理基础设施安全，例如我们如何混合云和物理硬件当我们需要连接事物像支付提供商，跨过专用线。

在其间，带来一些问题和评论，如果这些系统听起来像你感兴趣的事情，你也可以考虑加入团队。
